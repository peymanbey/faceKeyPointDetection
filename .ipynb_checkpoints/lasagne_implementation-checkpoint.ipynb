{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from faceKeySrc import load_data\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import lasagne\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just drop all the samples with missing values, consider a better approach\n"
     ]
    }
   ],
   "source": [
    "FTRAIN = '../data/training.csv'\n",
    "FTEST = '../data/test.csv'\n",
    "# load training data\n",
    "data_X, data_Y = load_data(FTRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate test validation split\n",
    "train_set_x, valid_set_x, train_set_y, valid_set_y = train_test_split(\n",
    "    data_X, data_Y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_x = train_set_x.astype(theano.config.floatX)\n",
    "train_set_y = train_set_y.astype(theano.config.floatX)\n",
    "valid_set_x = valid_set_x.astype(theano.config.floatX)\n",
    "valid_set_y = valid_set_y.astype(theano.config.floatX)\n",
    "#########################################\n",
    "# try it later to see if it improves the performance\n",
    "# # store traiing data into shared variable\n",
    "# train_set_x, train_set_y = shared_dataset(train_set_x, train_set_y)\n",
    "# valid_set_x, valid_set_y = shared_dataset(valid_set_x, valid_set_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear regression with multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build__model(width,\n",
    "                 num_output,\n",
    "                 drop_input= None,\n",
    "                 drop_hidden = None,\n",
    "                 input_var=None):\n",
    "\n",
    "    # Input layer and dropout (with shortcut `dropout` for `DropoutLayer`):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 9216),\n",
    "                                        input_var=input_var)\n",
    "    if drop_input:\n",
    "        network = lasagne.layers.dropout(network, p=drop_input)\n",
    "    # Hidden layers and dropout:\n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    for i in range(len(width)):\n",
    "        network = lasagne.layers.DenseLayer(\n",
    "            network, width[i], nonlinearity=nonlin)\n",
    "        if drop_hidden:\n",
    "            network = lasagne.layers.dropout(network, p=drop_hidden[0])\n",
    "            drop_hidden = drop_hidden[1:]\n",
    "        \n",
    "    # Output layer:\n",
    "    out_nonlin = lasagne.nonlinearities.identity\n",
    "    network = lasagne.layers.DenseLayer(network, num_output, nonlinearity=out_nonlin)\n",
    "        \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reinitiate_set_params(network,\n",
    "                          weights = None):\n",
    "        # change weights of a trained network to a random set or a user defined value\n",
    "        # useful in case of big networks and cross validation\n",
    "        # instead of the long time of recompiling you can just \n",
    "        # re-init the network weights\n",
    "        if not weights:\n",
    "            old = lasagne.layers.get_all_param_values(network)\n",
    "            weights = []\n",
    "            for layer in old:\n",
    "                shape = layer.shape\n",
    "                if len(shape)<2:\n",
    "                    shape = (shape[0], 1)\n",
    "                W= lasagne.init.GlorotUniform()(shape)\n",
    "                if W.shape != layer.shape:\n",
    "                    W = np.squeeze(W, axis= 1)\n",
    "                weights.append(W)\n",
    "        lasagne.layers.set_all_param_values(network, weights) \n",
    "        return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_update_functions(network,\n",
    "                           y,\n",
    "                           l2_reg = .1,\n",
    "                           learning_rate = .005,\n",
    "                           momentum = .9):\n",
    "    # build update functions\n",
    "    #####################################\n",
    "    # extract tensor representing the network predictions\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = 0\n",
    "    # collect squared error\n",
    "    loss = lasagne.objectives.squared_error(prediction, y)\n",
    "    # compute the root mean squared errror\n",
    "    loss = loss.mean().sqrt()\n",
    "    # add l2 regularization\n",
    "    l2_penalty = lasagne.regularization.regularize_network_params(network,\n",
    "                                                                  lasagne.regularization.l2)\n",
    "    loss = (1-l2_reg) * loss + l2_reg * l2_penalty\n",
    "    # get network params\n",
    "    params = lasagne.layers.get_all_params(network)\n",
    "    \n",
    "    # create update criterion    \n",
    "    print('nestrov')\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=learning_rate, momentum=momentum)\n",
    "    \n",
    "    # print('AdaGrad')\n",
    "    # updates = lasagne.updates.adagrad(loss, params)\n",
    "    \n",
    "    # create validation/test loss expression\n",
    "    # the loss represents the loss for all the lables\n",
    "    test_prediction = lasagne.layers.get_output(network,\n",
    "                                                deterministic=True)\n",
    "    # collect squared error\n",
    "    test_loss = lasagne.objectives.squared_error(test_prediction,\n",
    "                                                 y)\n",
    "    # compute the root mean squared errror\n",
    "    test_loss = test_loss.mean().sqrt()\n",
    "    # training function \n",
    "    \n",
    "    train_fn = theano.function(inputs=[X,y],\n",
    "                               outputs=loss,\n",
    "                               updates=updates)\n",
    "    # validation function \n",
    "    val_fn = theano.function(inputs=[X,y],\n",
    "                             outputs=[test_loss,prediction])\n",
    "    return train_fn,val_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = T.matrix('X')\n",
    "y = T.matrix('y')\n",
    "network = build__model([100,100,100],\n",
    "                       num_output=train_set_y.shape[1],\n",
    "                       drop_input=0,\n",
    "                       drop_hidden=None,\n",
    "                       input_var = X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nestrov\n"
     ]
    }
   ],
   "source": [
    "train_fn,val_fn = build_update_functions(network=network,\n",
    "                                         y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parasms \n",
    "n_iter = 3000\n",
    "improvement_threshold = 0.999\n",
    "patience = 10\n",
    "max_fail = 10\n",
    "########################\n",
    "n_fail_ = 0\n",
    "best_val_loss_ = np.inf\n",
    "best_epoch_ = 0 \n",
    "epoch = 0\n",
    "done_looping = False\n",
    "train_loss_history_ = []\n",
    "val_loss_history_ =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.0877171701277 epoch 200 n_fail 0\n"
     ]
    }
   ],
   "source": [
    "while (epoch < n_iter) and (not done_looping):\n",
    "            epoch += 1\n",
    "            if (epoch % 200) == 0:\n",
    "                print 'val loss',val_loss,'epoch',epoch,'n_fail',n_fail_\n",
    "\n",
    "            # one ful epoch on training data\n",
    "            train_loss = train_fn(train_set_x, train_set_y)\n",
    "            \n",
    "            # And a full pass over the validation data:\n",
    "            val_loss,_ = val_fn(valid_set_x, valid_set_y)\n",
    "                                   \n",
    "            # save trainin history\n",
    "            train_loss_history_.append(train_loss)\n",
    "            val_loss_history_.append(val_loss)\n",
    "\n",
    "            # check for early stopping\n",
    "\n",
    "            if np.mean(val_loss) < np.mean(best_val_loss_) * improvement_threshold:\n",
    "                best_val_loss_ = val_loss\n",
    "                best_epoch_ = epoch \n",
    "                best_network_params = lasagne.layers.get_all_param_values(network)\n",
    "                # you can later init a network with this weights by set_all_params_values\n",
    "                # it contains both W and b of all layers\n",
    "                n_fail_ = 0\n",
    "            elif (epoch % patience)==0:\n",
    "                n_fail_ += 1        \n",
    "            \n",
    "            if n_fail_ == max_fail : \n",
    "                done_looping = True\n",
    "                reinitiate_set_params(network=network,\n",
    "                                      weights = best_network_params)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(val_loss_history_);\n",
    "plt.plot(train_loss_history_);\n",
    "plt.legend(['val','train'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
