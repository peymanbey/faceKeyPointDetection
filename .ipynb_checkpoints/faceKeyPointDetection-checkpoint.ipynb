{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy\n",
    "from pandas.io.parsers import read_csv\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "FTRAIN = 'data/training.csv'\n",
    "FTEST = 'data/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  read inputs, handle missing values, scale input/output, convert to float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(path, test=False, col=None):\n",
    "    \"\"\" Load the data from path\n",
    "        by default it assums the training data and\n",
    "        loads all the columns\n",
    "    \"\"\"\n",
    "    df = read_csv(os.path.expanduser(path))\n",
    "    # the Image column is the pixel values separated by space\n",
    "    # convert the values to numpy array\n",
    "    df['Image'] = df['Image'].apply(lambda im: numpy.fromstring(im, sep=' '))\n",
    "    \n",
    "    # if you want only a subset of columns, passed as col to input\n",
    "    if col:\n",
    "        df = df[list(col)+['Image']]\n",
    "    \n",
    "    # some keypoints have missing valuses\n",
    "    # deal with them in handle_missing\n",
    "    # print(df.count())\n",
    "    df = handle_missing_values(df)\n",
    "    # print(df.count())\n",
    "    \n",
    "    # the Image column contains pixel values \n",
    "    # it is a list separated by space\n",
    "    # convert it into numpy array using np.vstack\n",
    "    # also scale them to [0, 1]\n",
    "    X = numpy.vstack(df['Image'].values) / 255.\n",
    "    \n",
    "    # convert values to float32\n",
    "    X = X.astype(numpy.float32)\n",
    "    \n",
    "    # for training data, manipulate target values\n",
    "    # scale the target values\n",
    "    # shuffle data\n",
    "    # Convert it to float 32\n",
    "    if not test:\n",
    "        Y = df[df.columns[:-1]].values\n",
    "        Y = (Y - 48) / 48  # scale target coordinates to [-1, 1]\n",
    "        X, Y = shuffle(X, Y, random_state =54)  # shuffle train data\n",
    "        Y = Y.astype(numpy.float32)\n",
    "    else:\n",
    "        Y = None\n",
    "\n",
    "    return X, Y\n",
    "    \n",
    "def handle_missing_values(df):\n",
    "    \"\"\"For the time being, just drop all the samples with missing values\n",
    "    \"\"\"\n",
    "    newdf= df.dropna()\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = load_data(FTRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(2140, 9216)\n",
      "19722240\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print X.shape\n",
    "print X.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(2140, 30)\n",
      "64200\n"
     ]
    }
   ],
   "source": [
    "print(type(Y))\n",
    "print Y.shape\n",
    "print Y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## implement a simple MLP for predictin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is based on Lisa-lab's tutorial on theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 960\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class linear_regresion(object):\n",
    "    \"\"\"multi-target linear regression \n",
    "    Fully described with weight matrix :math:'W'\n",
    "    and bias vectir :math:'b'.       \n",
    "    \"\"\"\n",
    "    def __init__(self, input, n_in,n_out):\n",
    "        \"\"\"initialize parameters of linear regression\n",
    "        :type input: theano.tensir.TensorType\n",
    "        :param input: the symbolic variable that describes\n",
    "        the input of the architecture (one minibatch)\n",
    "        \n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimesion of\n",
    "        the space data points lie in\n",
    "        \n",
    "        :type n_out: int \n",
    "        :param n_out: number of output units, the number of\n",
    "        target variables to predict\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # initializing the weghts matrix by zero and shape(n_in,n_out)\n",
    "        self.W= theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in,n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        # initialize bias\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "       \n",
    "        # symbolic expression of computing the output using W and b\n",
    "        self.y_pred=T.dot(input,self.W)+self.b# make sure it is correct\n",
    "        \n",
    "        # parameters of the model\n",
    "        self.param=[self.W,self.b]\n",
    "        \n",
    "        # keep track of model input\n",
    "        self.input=input\n",
    "        \n",
    "        # define the loss function\n",
    "    def loss_MSE(self,y):\n",
    "        \"\"\"returns the MSE error of prediction of the model\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: the vector that gives each samples correct prediction value\n",
    "        \"\"\"\n",
    "        #  T.sum(T.sqr(targets-outputs),axis=1) \n",
    "        # I use averaging to         \n",
    "        return T.mean(T.sqr(y-self.y_pred))#,axis=[0,1])\n",
    "    def errors(self, y):\n",
    "        \"\"\"return the number of errors in minibatch\n",
    "        \n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example \n",
    "        the correct target values\n",
    "        \"\"\"\n",
    "        # check if the dimension of y and y_pred is the same\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y',y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        return T.mean(T.neq(self.y_pred,y))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shared_dataset(X,y,borrow=True):\n",
    "    \"\"\"Load data into shared variables    \n",
    "    \"\"\"\n",
    "    shared_x=theano.shared(numpy.asarray(X,\n",
    "                                        dtype=theano.config.floatX),\n",
    "                          borrow=borrow)\n",
    "    shared_y= theano.shared(numpy.asarray(y,\n",
    "                                        dtype=theano.config.floatX),\n",
    "                          borrow=borrow)\n",
    "    return shared_x, shared_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set_x, train_set_y = shared_dataset(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sgd optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "n_epochs=10\n",
    "batch_size=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute the number of mini batches\n",
    "n_train_batches= train_set_x.get_value(borrow=True).shape[0] // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...building the model\n"
     ]
    }
   ],
   "source": [
    "print('...building the model')\n",
    "\n",
    "# allocate symbolic variable for data\n",
    "index = T.lscalar()\n",
    "\n",
    "# generate symbolic variable for data - x, y represent a single batch\n",
    "x= T.matrix('x')\n",
    "y=T.matrix('y')\n",
    "\n",
    "# construct the regressor\n",
    "linear_regressor = linear_regresion(input=x , n_in= 96 * 96, n_out=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the cost we minimise is MSE\n",
    "cost = linear_regressor.loss_MSE(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute the gradient of cost\n",
    "g_W = T.grad(cost=cost, wrt= linear_regressor.W)\n",
    "g_b = T.grad(cost=cost, wrt= linear_regressor.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how to update parameters of the model\n",
    "updates=[(linear_regressor.W, linear_regressor.W - learning_rate * g_W),\n",
    "        (linear_regressor.b, linear_regressor.b - learning_rate * g_b)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile a theano function that concurrently returns the cost\n",
    "# and updates the model based on the update rules in updates\n",
    "train_modle = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=cost,\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "        y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.153802663088\n",
      "0.0559129118919\n",
      "0.0164174567908\n",
      "0.0191400256008\n",
      "0.0132818818092\n",
      "0.0135677922517\n",
      "0.0189692396671\n",
      "0.0130664212629\n",
      "0.0148861864582\n",
      "0.0101540721953\n",
      "0.0124997589737\n",
      "0.0121159246191\n",
      "0.0140674570575\n",
      "0.0123562952504\n",
      "0.0110861454159\n",
      "0.0137573638931\n",
      "0.0125729553401\n",
      "0.0182141195983\n",
      "0.0158252939582\n",
      "0.0262765716761\n",
      "0.0260223709047\n",
      "0.0265084262937\n",
      "0.0312582552433\n",
      "0.00920404866338\n",
      "0.0163453239948\n",
      "0.0122031485662\n",
      "0.0127040781081\n",
      "0.01252489537\n",
      "0.0123019441962\n",
      "0.0134697100148\n",
      "0.0092889200896\n",
      "0.0107170939445\n",
      "0.0131232077256\n",
      "0.0113161727786\n",
      "0.0138591723517\n",
      "0.0116046490148\n",
      "0.017742857337\n",
      "0.01217188593\n",
      "0.014834147878\n",
      "0.0209081061184\n",
      "0.0178993213922\n",
      "0.0124277723953\n",
      "0.0186620410532\n",
      "0.0123948687688\n",
      "0.0119994236156\n",
      "0.00999766495079\n",
      "0.0102039705962\n",
      "0.0124348662794\n",
      "0.010773149319\n",
      "0.0188515353948\n",
      "0.0119586614892\n",
      "0.0146023565903\n",
      "0.00970216002315\n",
      "0.0110301440582\n",
      "0.0150132346898\n",
      "0.0110963117331\n",
      "0.017010319978\n",
      "0.011406602338\n",
      "0.0140414340422\n",
      "0.0136222913861\n",
      "0.0115389414132\n",
      "0.0151629680768\n",
      "0.0123283201829\n",
      "0.0103980349377\n",
      "0.0101435193792\n",
      "0.0168268550187\n",
      "0.0132685666904\n",
      "0.0128746144474\n",
      "0.0087810549885\n",
      "0.00616492750123\n",
      "0.00977735687047\n",
      "0.0154346497729\n",
      "0.0107304174453\n",
      "0.0163945555687\n",
      "0.0139012951404\n",
      "0.0185151360929\n",
      "0.0161593705416\n",
      "0.0141819408163\n",
      "0.0176029279828\n",
      "0.0101718064398\n",
      "0.0201172996312\n",
      "0.0153200412169\n",
      "0.0138742392883\n",
      "0.012464013882\n",
      "0.0150587437674\n",
      "0.0143284872174\n",
      "0.0246120411903\n",
      "0.0111511629075\n",
      "0.00928884651512\n",
      "0.00663049332798\n",
      "0.0093714511022\n",
      "0.00615833792835\n",
      "0.00984377693385\n",
      "0.0110495332628\n",
      "0.0095871957019\n",
      "0.00721586961299\n",
      "0.0153428949416\n",
      "0.0122482571751\n",
      "0.0182075761259\n",
      "0.0115973260254\n",
      "0.0136657087132\n",
      "0.0145232798532\n",
      "0.0130058312789\n",
      "0.0113224005327\n",
      "0.0106066381559\n",
      "0.0153358438984\n",
      "0.0142124993727\n",
      "0.0100655928254\n",
      "0.013800771907\n",
      "0.00839486997575\n",
      "0.0132646113634\n",
      "0.0111690312624\n",
      "0.0102016162127\n",
      "0.0145641490817\n",
      "0.0101727480069\n",
      "0.0120409457013\n",
      "0.00768501963466\n",
      "0.0107380710542\n",
      "0.0104495119303\n",
      "0.0126886880025\n",
      "0.0103363664821\n",
      "0.00914235599339\n",
      "0.0120362518355\n",
      "0.00990506447852\n",
      "0.0153866084293\n",
      "0.0134814018384\n",
      "0.0244234576821\n",
      "0.0209975279868\n",
      "0.0226197633892\n",
      "0.0273373331875\n",
      "0.00755806965753\n",
      "0.0148174073547\n",
      "0.0116483038291\n",
      "0.0107474802062\n",
      "0.0102829970419\n",
      "0.0109478635713\n",
      "0.011283243075\n",
      "0.00820708833635\n",
      "0.00945856422186\n",
      "0.0109943030402\n",
      "0.00942678004503\n",
      "0.0125187914819\n",
      "0.0100951753557\n",
      "0.0154476836324\n",
      "0.0103928092867\n",
      "0.0128149334341\n",
      "0.0184347517788\n",
      "0.014648988843\n",
      "0.0106587093323\n",
      "0.0154448086396\n",
      "0.0105622010306\n",
      "0.0110583305359\n",
      "0.00835439283401\n",
      "0.00860610790551\n",
      "0.0110117150471\n",
      "0.0095232045278\n",
      "0.0178441759199\n",
      "0.0103684617206\n",
      "0.0116370655596\n",
      "0.00835225358605\n",
      "0.00994196999818\n",
      "0.0124947270378\n",
      "0.00968457292765\n",
      "0.0149699524045\n",
      "0.0104833515361\n",
      "0.0121084693819\n",
      "0.0124626895413\n",
      "0.0107077574357\n",
      "0.0135003719479\n",
      "0.0101349893957\n",
      "0.0085673360154\n",
      "0.00907306652516\n",
      "0.0146565651521\n",
      "0.0114193847403\n",
      "0.010999170132\n",
      "0.00773210916668\n",
      "0.0050413729623\n",
      "0.00807252246886\n",
      "0.0138168605044\n",
      "0.00927080307156\n",
      "0.0148186581209\n",
      "0.0116857765242\n",
      "0.0158915407956\n",
      "0.0138139827177\n",
      "0.0118933757767\n",
      "0.0152206206694\n",
      "0.00925466790795\n",
      "0.0169706102461\n",
      "0.012994626537\n",
      "0.0119757410139\n",
      "0.010484370403\n",
      "0.0129515519366\n",
      "0.0119932247326\n",
      "0.0201329533011\n",
      "0.00913666468114\n",
      "0.00805732887238\n",
      "0.00598870078102\n",
      "0.00841441657394\n",
      "0.00546645512804\n",
      "0.00945619586855\n",
      "0.0104356147349\n",
      "0.00834557600319\n",
      "0.00612109107897\n",
      "0.0144328428432\n",
      "0.0120445042849\n",
      "0.0156617816538\n",
      "0.0104478336871\n",
      "0.0115711055696\n",
      "0.0130372848362\n",
      "0.01176411286\n",
      "0.0108126401901\n",
      "0.00973032787442\n",
      "0.013228581287\n",
      "0.012760267593\n",
      "0.0080389752984\n",
      "0.0117814308032\n",
      "0.00721110124141\n",
      "0.0119734471664\n",
      "0.0102727049962\n",
      "0.00874004513025\n",
      "0.0129275405779\n",
      "0.00912384409457\n",
      "0.0106204915792\n",
      "0.0064932028763\n",
      "0.00980099104345\n",
      "0.00959252007306\n",
      "0.0116029381752\n",
      "0.00919257849455\n",
      "0.00839785207063\n",
      "0.0111934244633\n",
      "0.00894260779023\n",
      "0.0135673899204\n",
      "0.0121737802401\n",
      "0.0231083780527\n",
      "0.0178076419979\n",
      "0.0196022875607\n",
      "0.0244190338999\n",
      "0.0070369867608\n",
      "0.0132838767022\n",
      "0.0111336531118\n",
      "0.00948370061815\n",
      "0.00923901144415\n",
      "0.0103396112099\n",
      "0.0103638814762\n",
      "0.00766366533935\n",
      "0.00854584295303\n",
      "0.0100626368076\n",
      "0.00872186757624\n",
      "0.0119076445699\n",
      "0.0092012854293\n",
      "0.013788336888\n",
      "0.00926266424358\n",
      "0.0115287806839\n",
      "0.0167975407094\n",
      "0.012700362131\n",
      "0.00976632162929\n",
      "0.01376726944\n",
      "0.0099961515516\n",
      "0.0104032214731\n",
      "0.00763809075579\n",
      "0.00767996348441\n",
      "0.0100461347029\n",
      "0.00876123830676\n",
      "0.0169261209667\n",
      "0.00966419931501\n",
      "0.0100406901911\n",
      "0.00757322181016\n",
      "0.00935839954764\n",
      "0.0111462110654\n",
      "0.00883162021637\n",
      "0.0134974718094\n",
      "0.00968236848712\n",
      "0.0109224049374\n",
      "0.0116701237857\n",
      "0.0098222438246\n",
      "0.0120065324008\n",
      "0.00889072660357\n",
      "0.00767226191238\n",
      "0.00846864655614\n",
      "0.0132821993902\n",
      "0.0104382168502\n",
      "0.00976070016623\n",
      "0.00708414614201\n",
      "0.00442212028429\n",
      "0.00721242465079\n",
      "0.0125426910818\n",
      "0.00818872731179\n",
      "0.0134705677629\n",
      "0.0101649584249\n",
      "0.0140990996733\n",
      "0.0123031688854\n",
      "0.0104233408347\n",
      "0.0136754363775\n",
      "0.00866276957095\n",
      "0.0150182833895\n",
      "0.0117064109072\n",
      "0.0113131953403\n",
      "0.0095112612471\n",
      "0.0114891109988\n",
      "0.0105730490759\n",
      "0.0173452924937\n",
      "0.00793557334691\n",
      "0.00732844183221\n",
      "0.0057354751043\n",
      "0.00767028238624\n",
      "0.00500628445297\n",
      "0.00923713669181\n",
      "0.00988535862416\n",
      "0.00754096405581\n",
      "0.00539857568219\n",
      "0.0137642202899\n",
      "0.0118219731376\n",
      "0.0138835404068\n",
      "0.00968036614358\n",
      "0.0103657916188\n",
      "0.0121381515637\n",
      "0.0107615701854\n",
      "0.0102835306898\n",
      "0.00908974837512\n",
      "0.0118193030357\n",
      "0.0116911251098\n",
      "0.00679870322347\n",
      "0.0106346989051\n",
      "0.0064951358363\n",
      "0.0110716670752\n",
      "0.00958303268999\n",
      "0.00787986628711\n",
      "0.0119560966268\n",
      "0.00862124282867\n",
      "0.00977036263794\n",
      "0.00571418507025\n",
      "0.009043850936\n",
      "0.00899065099657\n",
      "0.0108918696642\n",
      "0.00841211900115\n",
      "0.00792753417045\n",
      "0.0105343135074\n",
      "0.00826782174408\n",
      "0.0122170019895\n",
      "0.0112381530926\n",
      "0.0220540631562\n",
      "0.0156236803159\n",
      "0.0174102373421\n",
      "0.0220546163619\n",
      "0.00663740001619\n",
      "0.0121238911524\n",
      "0.0106059638783\n",
      "0.00873316824436\n",
      "0.00874465610832\n",
      "0.00999036990106\n",
      "0.00970991700888\n",
      "0.00726122083142\n",
      "0.00795647967607\n",
      "0.00949676241726\n",
      "0.00827009230852\n",
      "0.0114265875891\n",
      "0.00854006968439\n",
      "0.0125549836084\n",
      "0.0084599852562\n",
      "0.0105170663446\n",
      "0.0154891526327\n",
      "0.0113465609029\n",
      "0.00916932709515\n",
      "0.012652983889\n",
      "0.00970689021051\n",
      "0.00992000848055\n",
      "0.00723316241056\n",
      "0.00707078678533\n",
      "0.00935288425535\n",
      "0.00826093647629\n",
      "0.0161674283445\n",
      "0.00927357841283\n",
      "0.00896541494876\n",
      "0.00706057436764\n",
      "0.00891233235598\n",
      "0.0102923559025\n",
      "0.00822709314525\n",
      "0.0123690599576\n",
      "0.00906267482787\n",
      "0.0101528568193\n",
      "0.0110887680203\n",
      "0.00908996816725\n",
      "0.0107398554683\n",
      "0.00807349383831\n",
      "0.00711498130113\n",
      "0.00805367808789\n",
      "0.0123887816444\n",
      "0.00988532137126\n",
      "0.0088814869523\n",
      "0.0066383080557\n",
      "0.00403421325609\n",
      "0.00666333176196\n",
      "0.0115390671417\n",
      "0.00737677048892\n",
      "0.0123531138524\n",
      "0.00906204339117\n",
      "0.0127922287211\n",
      "0.0113000469282\n",
      "0.00939136277884\n",
      "0.0125585235655\n",
      "0.00822474528104\n",
      "0.0136036137119\n",
      "0.0108072627336\n",
      "0.0109928315505\n",
      "0.00886686611921\n",
      "0.0104224411771\n",
      "0.00958956964314\n",
      "0.0153953433037\n",
      "0.00709614809602\n",
      "0.00683730328456\n",
      "0.00560541031882\n",
      "0.00706534134224\n",
      "0.00469050742686\n",
      "0.00906057283282\n",
      "0.00944153405726\n",
      "0.00695946579799\n",
      "0.00489301839843\n",
      "0.0132448254153\n",
      "0.0116138160229\n",
      "0.0125653194264\n",
      "0.00911452993751\n",
      "0.00960790365934\n",
      "0.0115104261786\n",
      "0.00993682816625\n",
      "0.00978423189372\n",
      "0.00859889946878\n",
      "0.0108010033146\n",
      "0.0108648277819\n",
      "0.00594283593819\n",
      "0.00988749600947\n",
      "0.00602951226756\n",
      "0.0103760519996\n",
      "0.00907829310745\n",
      "0.00730507168919\n",
      "0.0112941740081\n",
      "0.00832024496049\n",
      "0.00918125640601\n",
      "0.00515270652249\n",
      "0.00843497831374\n",
      "0.00854071974754\n",
      "0.0104003269225\n",
      "0.00784387346357\n",
      "0.00758839258924\n",
      "0.0100062871352\n",
      "0.00775524834171\n",
      "0.0111707812175\n",
      "0.0105293542147\n",
      "0.0211965981871\n",
      "0.014058127068\n",
      "0.0157604273409\n",
      "0.020147299394\n",
      "0.00629531173036\n",
      "0.0112341167405\n",
      "0.0101246256381\n",
      "0.00825105234981\n",
      "0.00851195584983\n",
      "0.00976050738245\n",
      "0.00919918995351\n",
      "0.0069414828904\n",
      "0.00755943497643\n",
      "0.00910909380764\n",
      "0.00793235655874\n",
      "0.0110186841339\n",
      "0.00802762620151\n",
      "0.0116275586188\n",
      "0.00787456985563\n",
      "0.0097204213962\n",
      "0.0144330607727\n",
      "0.0103421099484\n",
      "0.00872535258532\n",
      "0.0118603380397\n",
      "0.00953457597643\n",
      "0.00954455323517\n",
      "0.00696872593835\n",
      "0.00663300929591\n",
      "0.0088347196579\n",
      "0.00790005829185\n",
      "0.0155285466462\n",
      "0.00902479235083\n",
      "0.00818439200521\n",
      "0.00669890688732\n",
      "0.00854715239257\n",
      "0.00970226619393\n",
      "0.00776602793485\n",
      "0.0114759523422\n",
      "0.00857415981591\n",
      "0.00963096134365\n",
      "0.010663215071\n",
      "0.00850159209222\n",
      "0.0096920998767\n",
      "0.00749121420085\n",
      "0.00673622684553\n",
      "0.00773447100073\n",
      "0.011773086153\n",
      "0.00955386925489\n",
      "0.00823572743684\n",
      "0.00631202990189\n",
      "0.00377144501545\n",
      "0.00626992015168\n",
      "0.0107385320589\n",
      "0.00674299290404\n",
      "0.0114397052675\n",
      "0.00822498649359\n",
      "0.0118081821129\n",
      "0.0105987573043\n",
      "0.00861566141248\n",
      "0.0117063531652\n",
      "0.00788063276559\n",
      "0.0125132128596\n",
      "0.0101204104722\n",
      "0.0108078988269\n",
      "0.008386047557\n",
      "0.00961007270962\n",
      "0.00886773038656\n",
      "0.013966107741\n",
      "0.00648388918489\n",
      "0.00648568803445\n",
      "0.00551800895482\n",
      "0.00657657487318\n",
      "0.00446747569367\n",
      "0.00891019403934\n",
      "0.00908351317048\n",
      "0.0065171411261\n",
      "0.00453089270741\n",
      "0.0128408782184\n",
      "0.0114337103441\n",
      "0.0115564046428\n",
      "0.0086805196479\n",
      "0.00910386163741\n",
      "0.0110345864668\n",
      "0.00926887895912\n",
      "0.00934235565364\n",
      "0.00822226889431\n",
      "0.0100300088525\n",
      "0.0102042909712\n",
      "0.00530862528831\n",
      "0.00935476180166\n",
      "0.00570521457121\n",
      "0.00980560388416\n",
      "0.00870006810874\n",
      "0.00689809350297\n",
      "0.0108142290264\n",
      "0.00811182521284\n",
      "0.00873304903507\n",
      "0.00472556473687\n",
      "0.00794234499335\n",
      "0.00819230265915\n",
      "0.0100384624675\n",
      "0.00740882428363\n",
      "0.0073232925497\n",
      "0.0095790354535\n",
      "0.00735624274239\n",
      "0.0103390468284\n",
      "0.00996939465404\n",
      "0.0204766653478\n",
      "0.0128823267296\n",
      "0.0144625380635\n",
      "0.0185826402158\n",
      "0.00600068597123\n",
      "0.01053402666\n",
      "0.00970736891031\n",
      "0.00791522767395\n",
      "0.00840890128165\n",
      "0.00959173031151\n",
      "0.00878333579749\n",
      "0.00667890720069\n",
      "0.00727797765285\n",
      "0.00881996937096\n",
      "0.00765736121684\n",
      "0.0106650516391\n",
      "0.00762261915952\n",
      "0.010918286629\n",
      "0.00743664242327\n",
      "0.00909348670393\n",
      "0.0135732013732\n",
      "0.00956495851278\n",
      "0.00837121158838\n",
      "0.0112757869065\n",
      "0.00942363310605\n",
      "0.00924161728472\n",
      "0.00677708769217\n",
      "0.00629959069192\n",
      "0.00843532197177\n",
      "0.0076229092665\n",
      "0.0149818873033\n",
      "0.00884560123086\n",
      "0.00759033905342\n",
      "0.00643097283319\n",
      "0.00824226438999\n",
      "0.00926630944014\n",
      "0.0073985834606\n",
      "0.0107571817935\n",
      "0.00818102248013\n",
      "0.00926494132727\n",
      "0.010351581499\n",
      "0.00802891235799\n",
      "0.00883261673152\n",
      "0.00705225998536\n",
      "0.00646513188258\n",
      "0.00747093884274\n",
      "0.0113275600597\n",
      "0.00933951046318\n",
      "0.00774647947401\n",
      "0.00606091180816\n",
      "0.00358286243863\n",
      "0.00596590340137\n",
      "0.0100925071165\n",
      "0.00623272825032\n",
      "0.0106903966516\n",
      "0.00756699265912\n",
      "0.0110469032079\n",
      "0.0100854430348\n",
      "0.00800373591483\n",
      "0.0110295685008\n",
      "0.00760176731274\n",
      "0.0116432458162\n",
      "0.00957050826401\n",
      "0.0106808366254\n",
      "0.00800238829106\n",
      "0.00897083710879\n",
      "0.00831638742238\n",
      "0.0128862932324\n",
      "0.00602426892146\n",
      "0.00622193189338\n",
      "0.00544421095401\n",
      "0.00618109945208\n",
      "0.0043047554791\n",
      "0.00877797137946\n",
      "0.00879153609276\n",
      "0.00616901973262\n",
      "0.00426548579708\n",
      "0.0125239724293\n",
      "0.011283012107\n",
      "0.0107682393864\n",
      "0.00833875127137\n",
      "0.00875114277005\n",
      "0.0106510473415\n",
      "0.00872996915132\n",
      "0.00895988382399\n",
      "0.0079306922853\n",
      "0.00942587666214\n",
      "0.00966316834092\n",
      "0.00481663225219\n",
      "0.00895201414824\n",
      "0.00546639086679\n",
      "0.00932190380991\n",
      "0.00840936601162\n",
      "0.00659704906866\n",
      "0.0104477833956\n",
      "0.00795008335263\n",
      "0.00837033428252\n",
      "0.00438766507432\n",
      "0.00753761036322\n",
      "0.00791373662651\n",
      "0.00975635554641\n",
      "0.00706235971302\n",
      "0.00710372580215\n",
      "0.00922848749906\n",
      "0.00703994883224\n",
      "0.00966571550816\n",
      "0.00951372366399\n",
      "0.0198564566672\n",
      "0.0119676114991\n",
      "0.0134067619219\n",
      "0.0172752607614\n",
      "0.00574581790715\n",
      "0.00997068639845\n",
      "0.00935045909137\n",
      "0.00766295753419\n",
      "0.00836685113609\n",
      "0.00945313833654\n",
      "0.0084350798279\n",
      "0.00645743636414\n",
      "0.00706815207377\n",
      "0.00858906190842\n",
      "0.0074194194749\n",
      "0.0103513868526\n",
      "0.00729805417359\n",
      "0.0103656537831\n",
      "0.0071001467295\n",
      "0.00859535206109\n",
      "0.0128642469645\n",
      "0.00894548743963\n",
      "0.00807516090572\n",
      "0.0108330957592\n",
      "0.0093477871269\n",
      "0.00898889638484\n",
      "0.00662488583475\n",
      "0.00603479146957\n",
      "0.00811852235347\n",
      "0.00740053132176\n",
      "0.0145074725151\n",
      "0.00870159827173\n",
      "0.00712405750528\n",
      "0.00622446415946\n",
      "0.0079837879166\n",
      "0.00892623979598\n",
      "0.00709685077891\n",
      "0.0101707959548\n",
      "0.00785831175745\n",
      "0.00900068227202\n",
      "0.0101208994165\n",
      "0.00764513621107\n",
      "0.00812594499439\n",
      "0.00670690974221\n",
      "0.00626312196255\n",
      "0.0072435750626\n",
      "0.0109914056957\n",
      "0.00918648950756\n",
      "0.00736441975459\n",
      "0.00585899828002\n",
      "0.00344104063697\n",
      "0.00571803702042\n",
      "0.0095640392974\n",
      "0.00581178534776\n",
      "0.0100686382502\n",
      "0.00703509384766\n",
      "0.0104435198009\n",
      "0.00969415996224\n",
      "0.00750425690785\n",
      "0.0104749919847\n",
      "0.00737053435296\n",
      "0.010932167992\n",
      "0.00911651924253\n",
      "0.0105777885765\n",
      "0.00768307596445\n",
      "0.00845457613468\n",
      "0.00788193754852\n",
      "0.0120506742969\n",
      "0.00567061686888\n",
      "0.00601507537067\n",
      "0.00537333451211\n",
      "0.00585798546672\n",
      "0.00418154662475\n",
      "0.0086580067873\n",
      "0.00854988023639\n",
      "0.00588771887124\n",
      "0.00406593689695\n",
      "0.0122708054259\n",
      "0.0111580742523\n",
      "0.0101422099397\n",
      "0.00806296803057\n",
      "0.00849123578519\n",
      "0.010327193886\n",
      "0.00829224847257\n",
      "0.00862918142229\n",
      "0.00770127680153\n",
      "0.00893948320299\n",
      "0.00921164266765\n",
      "0.00442292913795\n",
      "0.00863559730351\n",
      "0.00528226653114\n",
      "0.00890330504626\n",
      "0.00818025134504\n",
      "0.00636556372046\n",
      "0.0101540349424\n",
      "0.00781333073974\n",
      "0.00806422252208\n",
      "0.0041120601818\n",
      "0.00719865364954\n",
      "0.00768358726054\n",
      "0.00952477473766\n",
      "0.00677719479427\n",
      "0.00691430596635\n",
      "0.00893563404679\n",
      "0.00678467797115\n",
      "0.00911203585565\n",
      "0.00913443416357\n",
      "0.0193113926798\n",
      "0.011236988008\n",
      "0.0125269684941\n",
      "0.0161650814116\n",
      "0.00552363833413\n",
      "0.00950837507844\n",
      "0.00904374476522\n",
      "0.00746046751738\n",
      "0.0083490377292\n",
      "0.00932739209384\n",
      "0.008137059398\n",
      "0.00626611942425\n",
      "0.00690381973982\n",
      "0.0083944266662\n",
      "0.00720483856276\n",
      "0.0100664915517\n",
      "0.00703439582139\n",
      "0.00992665812373\n",
      "0.0068344315514\n",
      "0.00819329638034\n",
      "0.0122711770236\n",
      "0.00844025518745\n",
      "0.00781961344182\n",
      "0.0104901213199\n",
      "0.00929257180542\n",
      "0.00877164490521\n",
      "0.00649411696941\n",
      "0.00581752834842\n",
      "0.0078602489084\n",
      "0.00721576996148\n",
      "0.0140899717808\n",
      "0.00857522059232\n",
      "0.00674953358248\n",
      "0.00605967827141\n",
      "0.00776122417301\n",
      "0.00864896550775\n",
      "0.00684352451935\n",
      "0.0096860807389\n",
      "0.00758828502148\n",
      "0.00880462676287\n",
      "0.00994669646025\n",
      "0.00732842180878\n",
      "0.00754012400284\n",
      "0.00642580864951\n",
      "0.00610720692202\n",
      "0.00704196328297\n",
      "0.0107280975208\n",
      "0.00906423758715\n",
      "0.00705716619268\n",
      "0.00569045217708\n",
      "0.00333000090905\n",
      "0.00550809968263\n",
      "0.0091252932325\n",
      "0.0054575339891\n",
      "0.00954516604543\n",
      "0.00659514497966\n",
      "0.00995464157313\n",
      "0.00938500929624\n",
      "0.00708637107164\n",
      "0.0100092114881\n",
      "0.00717508187518\n",
      "0.0103398514912\n",
      "0.00873292330652\n",
      "0.0104833003134\n",
      "0.00740987854078\n",
      "0.00802867487073\n",
      "0.00753073068336\n",
      "0.0113906608894\n",
      "0.00539252953604\n",
      "0.00584581540897\n",
      "0.00530179124326\n",
      "0.00559024512768\n",
      "0.00408440129831\n",
      "0.00854625925422\n",
      "0.00834656599909\n",
      "0.00565543957055\n",
      "0.0039117494598\n",
      "0.012063652277\n",
      "0.011054106988\n",
      "0.00963745918125\n",
      "0.00783503334969\n",
      "0.00828941818327\n",
      "0.0100440056995\n",
      "0.0079320371151\n",
      "0.00834077876061\n",
      "0.00751690194011\n",
      "0.00853901728988\n",
      "0.00882941391319\n",
      "0.00410088384524\n",
      "0.00838034879416\n",
      "0.0051347669214\n",
      "0.00853597186506\n",
      "0.00799508579075\n",
      "0.00618117023259\n",
      "0.00990775693208\n",
      "0.00769072584808\n",
      "0.00779831316322\n",
      "0.00388173130341\n",
      "0.00690923258662\n",
      "0.0074873236008\n",
      "0.0093260128051\n",
      "0.00653583789244\n",
      "0.0067463782616\n",
      "0.0086860395968\n",
      "0.00657491292804\n",
      "0.0086501147598\n",
      "0.00881286896765\n",
      "0.0188247524202\n",
      "0.0106412339956\n",
      "0.0117805041373\n",
      "0.0152092780918\n",
      "0.00532812997699\n",
      "0.00912227574736\n",
      "0.00877695623785\n",
      "0.00728897145018\n",
      "0.00833626370877\n",
      "0.00920508895069\n",
      "0.00787761248648\n",
      "0.00609750626609\n",
      "0.00676897726953\n",
      "0.00822356715798\n",
      "0.00700620980933\n",
      "0.00980250071734\n",
      "0.00681715970859\n",
      "0.00957109127194\n",
      "0.00661900732666\n",
      "0.00786287244409\n",
      "0.0117677068338\n",
      "0.00802047085017\n",
      "0.0075939912349\n",
      "0.0102188829333\n",
      "0.00924942269921\n",
      "0.00857998616993\n",
      "0.00637485366315\n",
      "0.0056344922632\n",
      "0.00764428684488\n",
      "0.0070576495491\n",
      "0.0137175712734\n",
      "0.00845729280263\n",
      "0.00644323602319\n",
      "0.00592400925234\n",
      "0.00756669091061\n",
      "0.00841468758881\n",
      "0.00662711029872\n",
      "0.00928019266576\n",
      "0.00735829258338\n",
      "0.00865520071238\n",
      "0.00981152709574\n",
      "0.0070621361956\n",
      "0.00704920990393\n",
      "0.00619054213166\n",
      "0.00598299689591\n",
      "0.00686012301594\n",
      "0.0105147557333\n",
      "0.00895602162927\n",
      "0.00680310092866\n",
      "0.00554534373805\n",
      "0.00323994108476\n",
      "0.00532540073618\n",
      "0.00875555630773\n",
      "0.00515440292656\n",
      "0.00909774098545\n",
      "0.00622422574088\n",
      "0.00955049786717\n",
      "0.00913296826184\n",
      "0.00673017231748\n",
      "0.00961025804281\n",
      "0.007007161621\n",
      "0.00983869377524\n",
      "0.0084026362747\n",
      "0.0103904465213\n",
      "0.00717178871855\n",
      "0.00767105212435\n",
      "0.00724061531946\n",
      "0.0108597930521\n",
      "0.00516965333372\n",
      "0.00570201314986\n",
      "0.00522878393531\n",
      "0.00536492047831\n",
      "0.00400461396202\n",
      "0.00844020955265\n",
      "0.00817263778299\n",
      "0.00546012818813\n",
      "0.0037892642431\n",
      "0.0118896104395\n",
      "0.0109666371718\n",
      "0.00922475103289\n",
      "0.00764224678278\n",
      "0.0081246374175\n",
      "0.00978987105191\n",
      "0.00763072539121\n",
      "0.00808621011674\n",
      "0.00736516993493\n",
      "0.00820299517363\n",
      "0.00850206334144\n",
      "0.00383315281942\n",
      "0.00817039143294\n",
      "0.00501270731911\n",
      "0.00821027252823\n",
      "0.00784173421562\n",
      "0.00602951738983\n",
      "0.0096931187436\n",
      "0.00757671892643\n",
      "0.00756266713142\n",
      "0.00368547136895\n",
      "0.00665770564228\n",
      "0.00731521099806\n",
      "0.00914920121431\n",
      "0.0063267564401\n",
      "0.00659490190446\n",
      "0.00846910383552\n",
      "0.00639951368794\n",
      "0.0082594845444\n",
      "0.00853605940938\n",
      "0.018384821713\n",
      "0.0101472148672\n",
      "0.0111381327733\n",
      "0.0143766729161\n",
      "0.00515443272889\n",
      "0.00879474170506\n",
      "0.00854161288589\n",
      "0.00713774142787\n",
      "0.00831912085414\n",
      "0.00908158067614\n",
      "0.00764862075448\n",
      "0.00594655796885\n",
      "0.00665360083804\n",
      "0.00806910544634\n",
      "0.00681950431317\n",
      "0.00955430977046\n",
      "0.00663554156199\n",
      "0.00927756540477\n",
      "0.00644009932876\n",
      "0.00758630456403\n",
      "0.0113341920078\n",
      "0.00766622740775\n",
      "0.00739150587469\n",
      "0.0100003201514\n",
      "0.00921311415732\n",
      "0.00840725842863\n",
      "0.00626172823831\n",
      "0.00547686219215\n",
      "0.00745958276093\n",
      "0.00691887410358\n",
      "0.0133812120184\n",
      "0.0083431052044\n",
      "0.00618891837075\n",
      "0.00580911198631\n",
      "0.00739436317235\n",
      "0.00821106135845\n",
      "0.00643956847489\n",
      "0.00893597491086\n",
      "0.00715928105637\n",
      "0.0085381064564\n",
      "0.00970323663205\n",
      "0.00683404225856\n",
      "0.00663308892399\n",
      "0.00598905654624\n",
      "0.00588119588792\n",
      "0.00669436715543\n",
      "0.0103365955874\n",
      "0.00885294564068\n",
      "0.00658761430532\n",
      "0.00541730225086\n",
      "0.00316460756585\n",
      "0.00516327517107\n",
      "0.00843946821988\n",
      "0.00489136949182\n",
      "0.00870977621526\n",
      "0.00590646965429\n",
      "0.00921024288982\n",
      "0.00892185606062\n",
      "0.00642203260213\n",
      "0.00926305819303\n",
      "0.00686092022806\n",
      "0.0094090141356\n",
      "0.0081136925146\n",
      "0.0102963857353\n",
      "0.00696161575615\n",
      "0.00736626842991\n",
      "0.00699647655711\n",
      "0.0104256849736\n",
      "0.00498805288225\n",
      "0.00557603593916\n",
      "0.0051546022296\n",
      "0.00517236022279\n",
      "0.00393654918298\n",
      "0.00833840668201\n",
      "0.0080214580521\n",
      "0.00529336230829\n",
      "0.00368934357539\n",
      "0.0117394737899\n",
      "0.0108919115737\n",
      "0.00888276007026\n",
      "0.00747566111386\n",
      "0.00798391271383\n",
      "0.00955739337951\n",
      "0.00737420609221\n",
      "0.0078586647287\n",
      "0.00723717780784\n",
      "0.00791635829955\n",
      "0.00821897760034\n"
     ]
    }
   ],
   "source": [
    "# single run of the algorithm\n",
    "for iter in range(n_epochs): \n",
    "    for minibatch_index in range(n_train_batches):\n",
    "        minibatch_avg_cost = train_modle(minibatch_index)\n",
    "        print minibatch_avg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
