{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 960 (CNMeM is disabled, cuDNN 5005)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from faceKeySrc import shared_dataset,load_data\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import lasagne\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just drop all the samples with missing values, consider a better approach\n"
     ]
    }
   ],
   "source": [
    "FTRAIN = '../data/training.csv'\n",
    "FTEST = '../data/test.csv'\n",
    "# load training data\n",
    "data_X, data_Y = load_data(FTRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate test validation split\n",
    "train_set_x, valid_set_x, train_set_y, valid_set_y = train_test_split(\n",
    "    data_X, data_Y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set_x = train_set_x.astype(theano.config.floatX)\n",
    "train_set_y = train_set_y.astype(theano.config.floatX)\n",
    "valid_set_x = valid_set_x.astype(theano.config.floatX)\n",
    "valid_set_y = valid_set_y.astype(theano.config.floatX)\n",
    "#########################################\n",
    "# try it later to see if it improves the performance\n",
    "# store traiing data into shared variable\n",
    "train_set_x, train_set_y = shared_dataset(train_set_x, train_set_y)\n",
    "valid_set_x, valid_set_y = shared_dataset(valid_set_x, valid_set_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear regression with multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build__model(width,\n",
    "                 num_output,\n",
    "                 drop_input= None,\n",
    "                 drop_hidden = None,\n",
    "                 input_var=None):\n",
    "\n",
    "    # Input layer and dropout (with shortcut `dropout` for `DropoutLayer`):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 9216),\n",
    "                                        input_var=input_var)\n",
    "    if drop_input:\n",
    "        \n",
    "        network = lasagne.layers.dropout(network, p=drop_input)\n",
    "    # Hidden layers and dropout:\n",
    "    nonlin = lasagne.nonlinearities.LeakyRectify(.1)\n",
    "    for i in range(len(width)):\n",
    "        network = lasagne.layers.DenseLayer(\n",
    "            network, width[i], nonlinearity=nonlin)\n",
    "        if drop_hidden:\n",
    "            network = lasagne.layers.dropout(network, p=drop_hidden[0])\n",
    "            drop_hidden = drop_hidden[1:]\n",
    "        \n",
    "    # Output layer:\n",
    "    out_nonlin = lasagne.nonlinearities.identity\n",
    "    network = lasagne.layers.DenseLayer(network, num_output, nonlinearity=out_nonlin)\n",
    "        \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reinitiate_set_params(network,\n",
    "                          weights = None):\n",
    "        # change weights of a trained network to a random set or a user defined value\n",
    "        # useful in case of big networks and cross validation\n",
    "        # instead of the long time of recompiling you can just \n",
    "        # re-init the network weights\n",
    "        if not weights:\n",
    "            old = lasagne.layers.get_all_param_values(network)\n",
    "            weights = []\n",
    "            for layer in old:\n",
    "                shape = layer.shape\n",
    "                if len(shape)<2:\n",
    "                    shape = (shape[0], 1)\n",
    "                W= lasagne.init.GlorotUniform()(shape)\n",
    "                if W.shape != layer.shape:\n",
    "                    W = np.squeeze(W, axis= 1)\n",
    "                weights.append(W)\n",
    "        lasagne.layers.set_all_param_values(network, weights) \n",
    "        return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_update_functions(train_set_x,train_set_y,\n",
    "                           valid_set_x,valid_set_y,\n",
    "                           network,\n",
    "                           y,\n",
    "                           batch_size = 32,\n",
    "                           l2_reg = .01,\n",
    "                           learning_rate = .005,\n",
    "                           momentum = .9):\n",
    "    # build update functions\n",
    "    #####################################\n",
    "    # extract tensor representing the network predictions\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss_RMSE = 0\n",
    "    # collect squared error\n",
    "    loss_RMSE = lasagne.objectives.squared_error(prediction, y)\n",
    "    # compute the root mean squared errror\n",
    "    loss_RMSE = loss_RMSE.mean().sqrt()\n",
    "    # add l2 regularization\n",
    "    l2_penalty = lasagne.regularization.regularize_network_params(network,\n",
    "                                                                  lasagne.regularization.l2)\n",
    "    loss = (1-l2_reg) * loss_RMSE + l2_reg * l2_penalty\n",
    "    # get network params\n",
    "    params = lasagne.layers.get_all_params(network)\n",
    "    \n",
    "#     # create update criterion    \n",
    "#     print('nestrov')\n",
    "#     updates = lasagne.updates.nesterov_momentum(\n",
    "#         loss, params, learning_rate=learning_rate, momentum=momentum)\n",
    "    \n",
    "#     print('AdaGrad')\n",
    "#     updates = lasagne.updates.adagrad(loss, params,learning_rate= 1e-3)\n",
    "    \n",
    "    print('RMSPROP')\n",
    "    updates = lasagne.updates.rmsprop(loss, params, learning_rate= 1e-3)\n",
    "    # create validation/test loss expression\n",
    "    # the loss represents the loss for all the lables\n",
    "    test_prediction = lasagne.layers.get_output(network,\n",
    "                                                deterministic=True)\n",
    "    # collect squared error\n",
    "    test_loss = lasagne.objectives.squared_error(test_prediction,\n",
    "                                                 y)\n",
    "    # compute the root mean squared errror\n",
    "    test_loss = test_loss.mean().sqrt()\n",
    "#     test_loss_withl2 = (1-l2_reg) * test_loss + l2_reg * l2_penalty\n",
    "    \n",
    "    # index for minibatch slicing\n",
    "    index = T.lscalar()\n",
    "    \n",
    "    # training function \n",
    "    train_set_x_size = train_set_x.get_value().shape[0]\n",
    "    val_set_x_size = valid_set_x.get_value().shape[0]\n",
    "    \n",
    "    train_fn = theano.function(inputs=[index],\n",
    "                               outputs=[loss,loss_RMSE],\n",
    "                               updates=updates,\n",
    "                               givens={X:train_set_x[index * batch_size: T.minimum((index + 1) * batch_size,train_set_x_size)],\n",
    "                                       y:train_set_y[index * batch_size: T.minimum((index + 1) * batch_size,train_set_x_size)]})\n",
    "    # validation function \n",
    "    val_fn = theano.function(inputs=[index],\n",
    "                             outputs=[test_loss,prediction],\n",
    "                             givens={X:valid_set_x[index * batch_size: T.minimum((index + 1) * batch_size,val_set_x_size)],\n",
    "                                     y:valid_set_y[index * batch_size: T.minimum((index + 1) * batch_size,val_set_x_size)]})\n",
    "    return train_fn,val_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if type(train_set_y) == np.ndarray:\n",
    "    num_output = train_set_y.shape[1]\n",
    "else:\n",
    "    num_output = train_set_y.get_value().shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = T.matrix('X')\n",
    "y = T.matrix('y')\n",
    "network = build__model([1024,1024,512,128],\n",
    "                       num_output=num_output,\n",
    "                       drop_input=.05,\n",
    "                       drop_hidden=[.1,.1,.2,.2],\n",
    "                       input_var = X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSPROP\n"
     ]
    }
   ],
   "source": [
    "train_fn,val_fn = build_update_functions(train_set_x,train_set_y,\n",
    "                                         valid_set_x,valid_set_y,\n",
    "                                         network,\n",
    "                                         y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parasms \n",
    "n_iter = 10000\n",
    "improvement_threshold = 0.999\n",
    "patience = 10000\n",
    "max_fail = 10\n",
    "#######################\n",
    "batch_size = 128\n",
    "n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size+1\n",
    "n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size+1\n",
    "patience_increase=.2\n",
    "validation_frequency = min(n_train_batches, patience // 10)\n",
    "best_val_loss_ = np.inf\n",
    "train_loss_history_temp = []\n",
    "########################\n",
    "n_fail_ = 0\n",
    "best_val_loss_ = np.inf\n",
    "best_epoch_ = 0 \n",
    "epoch = 0\n",
    "done_looping = False\n",
    "train_loss_history_ = []\n",
    "val_loss_history_ =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1712\n",
      "428\n"
     ]
    }
   ],
   "source": [
    "print train_set_x.get_value().shape[0]\n",
    "print valid_set_x.get_value().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "while (epoch < n_iter) and (not done_looping):\n",
    "    epoch +=1\n",
    "    \n",
    "    # go over minibatches\n",
    "    for minibatch_index in range(n_train_batches):\n",
    "        \n",
    "        # update network for one minibatch\n",
    "        minibatch_average_cost,minibatch_average_RMSE = train_fn(minibatch_index)\n",
    "        \n",
    "        # store training loss of minibatches till the next validation step\n",
    "        train_loss_history_temp.append(minibatch_average_RMSE)\n",
    "        \n",
    "        # number of minibatches checked\n",
    "        num_minibatch_checked = (epoch -1) * n_train_batches + minibatch_index\n",
    "        \n",
    "        # if validation interval reached\n",
    "        if (num_minibatch_checked+1 ) % validation_frequency ==0:\n",
    "            \n",
    "            # compute validation loss\n",
    "            validation_losses = [val_fn(i)[0] for i in range(n_valid_batches)]\n",
    "            \n",
    "            # store mean validation loss for validation set\n",
    "            current_val_loss = np.mean(validation_losses)\n",
    "            \n",
    "            # store training and validation history\n",
    "            train_loss_history_.append(np.mean(train_loss_history_temp))\n",
    "            val_loss_history_.append(current_val_loss)\n",
    "            train_loss_history_temp = []\n",
    "\n",
    "            # is it the best validation loss sofar?\n",
    "            if current_val_loss < best_val_loss_ :\n",
    "                \n",
    "                # increase patience if improvement is significant\n",
    "                if (current_val_loss < best_val_loss_*improvement_threshold):\n",
    "                    patience = max(patience, num_minibatch_checked* patience_increase )\n",
    "                \n",
    "                # save the-sofar-best validation RMSE and epoch and model-params\n",
    "                best_val_loss_ = current_val_loss\n",
    "                best_epoch_ = epoch\n",
    "                best_network_params = lasagne.layers.get_all_param_values(network)\n",
    "                \n",
    "        # check if patience exceeded and set the training loop to stop\n",
    "        if (patience <= num_minibatch_checked):\n",
    "            print 'patience reached'\n",
    "            # reset the network weights to the best params saved\n",
    "            print 'reseting the network params to that of the best seen'\n",
    "            reinitiate_set_params(network=network,\n",
    "                                  weights = best_network_params)\n",
    "            # done optimising, break the optimisation loop\n",
    "            done_looping = True\n",
    "            break\n",
    "   \n",
    "    # print out results of optimising priodically\n",
    "#     if num_minibatch_checked<1000:\n",
    "#         freq = 200\n",
    "#     elif num_minibatch_checked<5000:\n",
    "#         freq = 500\n",
    "#     elif num_minibatch_checked<10000:\n",
    "#         freq = 1000\n",
    "    freq = 200\n",
    "        \n",
    "    if (epoch % freq) == 0:\n",
    "        print (('epoch %i, minibatch %i/%i, validation loss of %f, patience %i, in %f secs')%\n",
    "               (epoch, minibatch_index+1, n_train_batches, current_val_loss, patience, time.time()-start_time ))\n",
    "        start_time = time.time()\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "plt.plot(val_loss_history_[-1000:]);\n",
    "plt.plot(train_loss_history_[-1000:]);\n",
    "plt.legend(['val','train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make a submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_fn(train_set_x,train_set_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
